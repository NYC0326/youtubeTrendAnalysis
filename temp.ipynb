{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing channel UCTheFErn4MureanSiqgKIag: <HttpError 404 when requesting https://youtube.googleapis.com/youtube/v3/playlistItems?part=contentDetails&playlistId=UUSHTheFErn4MureanSiqgKIag&maxResults=50&key=AIzaSyCips5I73F9bykzcN6nhek4xcmzS6HhUfU&alt=json returned \"The playlist identified with the request's <code>playlistId</code> parameter cannot be found.\". Details: \"[{'message': \"The playlist identified with the request's <code>playlistId</code> parameter cannot be found.\", 'domain': 'youtube.playlistItem', 'reason': 'playlistNotFound', 'location': 'playlistId', 'locationType': 'parameter'}]\">\n",
      "Error processing channel UCU2zNeYhf9pi_wSqFbYE96w: <HttpError 404 when requesting https://youtube.googleapis.com/youtube/v3/playlistItems?part=contentDetails&playlistId=UUSHU2zNeYhf9pi_wSqFbYE96w&maxResults=50&key=AIzaSyCips5I73F9bykzcN6nhek4xcmzS6HhUfU&alt=json returned \"The playlist identified with the request's <code>playlistId</code> parameter cannot be found.\". Details: \"[{'message': \"The playlist identified with the request's <code>playlistId</code> parameter cannot be found.\", 'domain': 'youtube.playlistItem', 'reason': 'playlistNotFound', 'location': 'playlistId', 'locationType': 'parameter'}]\">\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "\n",
    "load_dotenv()\n",
    "# API key settings\n",
    "API_KEY = os.getenv('youtube_api_key_3')\n",
    "youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "df = pd.read_csv('youtubeChannelData.csv')\n",
    "\n",
    "# Only top 10 channels for testing (Quota limit)\n",
    "df_channels = df[:10]\n",
    "\n",
    "# Datas that will be collected\n",
    "columns = ['Channel Id', 'Channel Name', 'Category', 'publishedAt', 'title', 'description', \n",
    "          'channelTitle', 'categoryId', 'viewCount', 'likeCount', 'commentCount']\n",
    "df_videos = pd.DataFrame(columns=columns)\n",
    "\n",
    "def get_shorts_videos(playlist_id):\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while True:\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"contentDetails\",\n",
    "            playlistId=playlist_id,\n",
    "            maxResults=50,\n",
    "            pageToken=next_page_token\n",
    "        )\n",
    "        response = request.execute()\n",
    "\n",
    "        # get video id from playlist\n",
    "        for item in response.get('items', []):\n",
    "            video_id = item['contentDetails']['videoId']\n",
    "            videos.append(video_id)\n",
    "\n",
    "        # check if there is next page\n",
    "        next_page_token = response.get('nextPageToken')\n",
    "        if not next_page_token:\n",
    "            break\n",
    "\n",
    "    return videos\n",
    "\n",
    "def get_videos_metadata(video_ids):\n",
    "    all_video_data = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50):  # API max limit is 50\n",
    "        request = youtube.videos().list(\n",
    "            part=\"snippet,contentDetails,statistics\",\n",
    "            id=','.join(video_ids[i:i+50]),\n",
    "            fields=\"items(id,snippet(publishedAt,title,description,channelTitle,categoryId),statistics(viewCount,likeCount,commentCount))\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Filter out videos that are not from 2024\n",
    "        for video in response.get('items', []):\n",
    "            published_at = video['snippet']['publishedAt']\n",
    "            if published_at[:4] != '2024':\n",
    "                break  # Playlist is sorted by date, so if it's before 2024, we can stop here\n",
    "            \n",
    "            video_data = {\n",
    "                'publishedAt': published_at,\n",
    "                'title': video['snippet']['title'],\n",
    "                'description': video['snippet']['description'],\n",
    "                'channelTitle': video['snippet']['channelTitle'],\n",
    "                'categoryId': video['snippet']['categoryId'],\n",
    "                'viewCount': video['statistics'].get('viewCount', 0),\n",
    "                'likeCount': video['statistics'].get('likeCount', 0),\n",
    "                'commentCount': video['statistics'].get('commentCount', 0)\n",
    "            }\n",
    "            all_video_data.append(video_data)\n",
    "\n",
    "    return pd.DataFrame(all_video_data)\n",
    "\n",
    "for _, channel in df_channels.iterrows():\n",
    "    try:\n",
    "        # getting Shorts Playlist ID\n",
    "        channel_id = channel['Channel ID']\n",
    "        playlist_id = 'UUSH' + channel_id[2:]  # replace first 2 characters with 'UUSH'\n",
    "\n",
    "        # get shorts videos from playlist\n",
    "        video_ids = get_shorts_videos(playlist_id)\n",
    "        if not video_ids:\n",
    "            continue\n",
    "\n",
    "        # get metadata of videos\n",
    "        videos_metadata = get_videos_metadata(video_ids)\n",
    "        \n",
    "        # add channel info\n",
    "        videos_metadata['Channel Id'] = channel['Channel ID']\n",
    "        videos_metadata['Channel Name'] = channel['Channel Name']\n",
    "        videos_metadata['Category'] = channel['Category']\n",
    "\n",
    "        # add to dataframe\n",
    "        df_videos = pd.concat([df_videos, videos_metadata], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing channel {channel['Channel ID']}: {e}\") # Quota Error or No shorts videos\n",
    "        continue\n",
    "\n",
    "# save to csv\n",
    "df_videos.to_csv('youtube_videos_2024.csv', index=False, encoding='utf-8-sig')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_channels = df[20:30]\n",
    "\n",
    "df_videos = pd.DataFrame(columns=columns)\n",
    "\n",
    "for _, channel in df_channels.iterrows():\n",
    "    try:\n",
    "        # getting Shorts Playlist ID\n",
    "        channel_id = channel['Channel ID']\n",
    "        playlist_id = 'UUSH' + channel_id[2:]  # replace first 2 characters with 'UUSH'\n",
    "\n",
    "        # get shorts videos from playlist\n",
    "        video_ids = get_shorts_videos(playlist_id)\n",
    "        if not video_ids:\n",
    "            continue\n",
    "\n",
    "        # get metadata of videos\n",
    "        videos_metadata = get_videos_metadata(video_ids)\n",
    "        \n",
    "        # add channel info\n",
    "        videos_metadata['Channel Id'] = channel['Channel ID']\n",
    "        videos_metadata['Channel Name'] = channel['Channel Name']\n",
    "        videos_metadata['Category'] = channel['Category']\n",
    "\n",
    "        # add to dataframe\n",
    "        df_videos = pd.concat([df_videos, videos_metadata], ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing channel {channel['Channel ID']}: {e}\")\n",
    "        continue\n",
    "\n",
    "# save to csv by adding to existing csv\n",
    "existing_csv_path = 'youtube_videos_2024.csv'\n",
    "df_videos.to_csv(existing_csv_path, mode='a', header=False, index=False, encoding='utf-8-sig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "youtubeDataTrendAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
